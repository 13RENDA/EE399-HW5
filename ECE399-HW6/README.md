# ECE399-HW6

### Author: Shiyu Chen

## Abstract
In this project, we will use SHRED (SHallow REcurrent Decoder)  model to process data on Sea Surface Temperature (SST) and do analysis of its performance with varied lag values and number of sensors. We will also examine the model’s performance with noisy data.

## Introduction

This report explore the utilization of SHRED models for the reconstruction of SST dataset. Our aim is to evaluate the performance as a function of lag variable, noise, and the number of sensors. To achieve this goal, we will compare the Mean Square Error (mse) in each round of tests and use plot to visualize the result.

Theoretical background

**SHRED** (SHallow REcurrent Decoder) models are a specific type of network architecture that combines a recurrent layer, typically LSTM (Long Short-Term Memory), with a shallow decoder network, known as SDN. The main purpose of SHRED models is to reconstruct high-dimensional spatio-temporal fields using a sequence of sensor measurements from the field. By merging the recurrent layer and the shallow decoder network, SHRED models can effectively capture temporal dependencies in the data and generate accurate reconstructions of the spatio-temporal fields. The recurrent layer, such as LSTM, handles the temporal modeling and encodes the relevant information from the input sequence, while the shallow decoder network decodes this information and reconstructs the high-dimensional spatio-temporal fields. This architecture allows SHRED models to leverage the strengths of both components, resulting in improved performance in reconstructing complex and dynamic spatio-temporal fields from sensor measurements.

## Algorithm Implementation

```
def training(num_sensors,lags,load_X,n,m,sensor_locations):
 train_indices = np.random.choice(n - lags, size=1000, replace=False)
 mask = np.ones(n - lags)
 mask[train_indices] = 0
 valid_test_indices = np.arange(0, n - lags)[np.where(mask!=0)[0]]
 valid_indices = valid_test_indices[::2]
 test_indices = valid_test_indices[1::2]


 sc = MinMaxScaler()
 sc = sc.fit(load_X[train_indices])
 transformed_X = sc.transform(load_X)


 ### Generate input sequences to a SHRED model
 all_data_in = np.zeros((n - lags, lags, num_sensors))
 for i in range(len(all_data_in)):
     all_data_in[i] = transformed_X[i:i+lags, sensor_locations]


```
The main function of this project is called training that performs data processing steps for training SHRED model. It takes the number of sensors, lags, load_X dataset, dimension of the data set (n,m), and the sensor locations as parameters. With this function, we can easily perform the training process multiple times with different parameters. 

The function starts by randomly selecting 1000 indices from the range of available data, which will be used for training. The remaining indices are divided into validation and test sets. The code then applies a MinMaxScaler to normalize the load_X dataset based on the training indices. This scaler is fitted on the training subset to capture the range of the data. Subsequently, the normalized data is stored in transformed_X. The code proceeds to create input sequences for the SHRED model by constructing the all_data_in array. Each input sequence consists of a specified number of consecutive time steps (defined by lags) and selected sensor locations. These input sequences serve as the training data for the SHRED model.

```
 train_dataset = TimeSeriesDataset(train_data_in, train_data_out)
 valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)
 test_dataset = TimeSeriesDataset(test_data_in, test_data_out)


 shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)


 test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())
 test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())
```
We build training, valid, and testing datasets, each of which contains an input-output dataset for LSTM model training. Then initialize SHRED model for training. Test_recons collects the result of testing generated by SHRED model and test_ground_truth is the real data.

```
# SST data with world map indices for plotting
 full_SST, sst_locs = load_full_SST()
 full_test_truth = full_SST[test_indices, :]


 # replacing SST data with our reconstruction
 full_test_recon = full_test_truth.copy()
 full_test_recon[:,sst_locs] = test_recons
 …
 …
 mse = np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth)
 
```
Reconstructing data and computing test error (Mean Square Error(mse)) for plotting.

```
def training2(num_sensors,lags,load_X,n,m,sensor_locations):
…


# Define the list of lag values
lag_val = [13, 55, 65, 95, 100, 113, 125, 132, 170, 199]
num_sensors = 3
mse_errors = []


for lag in lag_val:
   mse = training2(num_sensors,lags,load_X,n,m,sensor_locations)
   mse_errors.append(mse)
  


# Plot MSE errors vs lags
plt.figure()
plt.plot(lag_val, mse_errors, marker='o')
plt.xlabel("Lag")
plt.ylabel("MSE Error")
plt.title("MSE Errors For Different Lag Values")
plt.show()

```
Training2 has the same usage as training just abandon the plotting section.

Change the number of lag for training and make a plot for analysing the performance of SHRED model.

```
sensor_num = [1, 3, 5, 6, 8, 11, 12, 16, 17, 20]
lag = 52
mse_errors = []


for num_sensors in sensor_num:
   sensor_locations = np.random.choice(m, size=num_sensors, replace=False)
   mse = training2(num_sensors,lag,load_X,n,m,sensor_locations)
   mse_errors.append(mse)
```
Make the number of sensors a variable.

```
# Parameters for Gaussian noise
mean = 0  # Mean of the Gaussian distribution
num_sensors = 3
lags = 52
sensor_locations = np.random.choice(m, size=num_sensors, replace=False)


# Add Gaussian noise to data
noise_std = np.linspace(0, 0.5, num=10)
errors = []
for std in noise_std:
 load_X_noise = load_X + np.random.normal(mean, std, size=load_X.shape)
 e = training2(num_sensors,lags,load_X_noise,n,m,sensor_locations)
 errors.append(e)
```
Add Gaussian noise to data, and train the model with the noisy data set. 

## Result

### SHRED model in reconstructing the images

**Test Error: 0.2197908**

![image](https://github.com/13RENDA/ECE399-HW6/assets/122130043/e21d4844-0b14-4b1c-8418-6e71e974d459)

### Performance of SHRED model as a function of number of lags

![image](https://github.com/13RENDA/ECE399-HW6/assets/122130043/bdf4165d-fab9-4880-a8ba-dd2e2817e88d)

According to the plot, there is no obvious correlation between the number of lags and the MSE error. The lowest error happened when the lag equals 125.

### Performance of SHRED model with noisy data

![image](https://github.com/13RENDA/ECE399-HW6/assets/122130043/78923d07-e2b4-4b1a-85fa-762ac77bf26d)

**SHRED model in reconstructing the image with noisy data**

![image](https://github.com/13RENDA/ECE399-HW6/assets/122130043/c116a5e5-5311-46fd-bf1d-6d1efee82175)

According to the plot, we can see that there is a positive correlation between noise std and test error, which implies that the model will be influenced by noise in the dataset.

### Performance of SHRED model as a function of number of sensors

![image](https://github.com/13RENDA/ECE399-HW6/assets/122130043/ed1cfa42-4888-40db-bf0a-4ef71d82761c)

From the plot, there is a negative correlation between the number of sensors and the mse error from sensor number is larger than 5. The testing error reaches its largest value when there are 5 sensors but gradually goes down with increasing sensor numbers.

## Conclusion

In conclusion, SHRED model has high accuracy when performing on noise-free data but will be influenced by noise in the dataset. Additionally, the number of sensors exhibits notable impace to the model. With an increasing number of sensors, there was a decrease in the mse, indicating enhanced accuracy in forecasting. Conversrly, the varying lag value didn’t display any significant correlation to the model.






