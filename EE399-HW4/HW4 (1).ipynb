{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Author: Shiyu Chen\n",
        "# Date: May 8th, 2023\n",
        "# Github: https://github.com/13RENDA/EE399-HW4.git"
      ],
      "metadata": {
        "id": "2_AxISYh7lOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ewq_i4V6-gLW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Lasso\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5rAFdim-gLa"
      },
      "outputs": [],
      "source": [
        "# Declear variables\n",
        "X = np.arange(0,31)\n",
        "Y = np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41, 40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOCBJa_i-gLb"
      },
      "outputs": [],
      "source": [
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(1,10)\n",
        "        self.fc2 = nn.Linear(10,10)\n",
        "        self.fc3 = nn.Linear(10,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.view(-1, 1) # flatten the input image\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_t = torch.tensor(X, dtype=torch.float32).reshape(-1, 1)\n",
        "Y_t = torch.tensor(Y, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Initialize the neural network\n",
        "net = Net()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 100\n",
        "# Train the neural network\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(X_t.shape[0]):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(X_t[i])\n",
        "        loss = criterion(outputs, Y_t[i])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 10 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, X_t.shape[0], loss.item()))\n"
      ],
      "metadata": {
        "id": "srDwjwTApi0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test = X[:20], X[20:]\n",
        "Y_train, Y_test = Y[:20], Y[20:]\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32).reshape(-1, 1)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32).reshape(-1, 1)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.float32).reshape(-1, 1)\n",
        "# Initialize the neural network\n",
        "net = Net()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 10\n",
        "# Train the neural network\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(X_train.shape[0]):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(X_train[i])\n",
        "        loss = criterion(outputs, Y_train[i])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 10 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, X_train.shape[0], loss.item()))\n",
        "\n",
        "\n",
        "# Compute the least square error for each data point in the test set\n",
        "Y_test_pred = net(X_test).detach().numpy().flatten()\n",
        "test_error = np.mean((Y_test_pred - Y_test.numpy())**2)\n",
        "print('Test Error: {:.4f}'.format(test_error))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VFi436SkBd_",
        "outputId": "53306e73-5e24-474f-eb57-aa9054261cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/20], Loss: 1240.9994\n",
            "Epoch [1/10], Step [20/20], Loss: 1402.3779\n",
            "Epoch [2/10], Step [10/20], Loss: 881.0172\n",
            "Epoch [2/10], Step [20/20], Loss: 371.1078\n",
            "Epoch [3/10], Step [10/20], Loss: 248.6775\n",
            "Epoch [3/10], Step [20/20], Loss: 201.3156\n",
            "Epoch [4/10], Step [10/20], Loss: 24.6263\n",
            "Epoch [4/10], Step [20/20], Loss: 369.3309\n",
            "Epoch [5/10], Step [10/20], Loss: 82.0394\n",
            "Epoch [5/10], Step [20/20], Loss: 164.5789\n",
            "Epoch [6/10], Step [10/20], Loss: 58.6802\n",
            "Epoch [6/10], Step [20/20], Loss: 224.1370\n",
            "Epoch [7/10], Step [10/20], Loss: 61.9673\n",
            "Epoch [7/10], Step [20/20], Loss: 165.8345\n",
            "Epoch [8/10], Step [10/20], Loss: 48.4453\n",
            "Epoch [8/10], Step [20/20], Loss: 179.7131\n",
            "Epoch [9/10], Step [10/20], Loss: 43.9369\n",
            "Epoch [9/10], Step [20/20], Loss: 157.1173\n",
            "Epoch [10/10], Step [10/20], Loss: 39.8861\n",
            "Epoch [10/10], Step [20/20], Loss: 142.1545\n",
            "Test Error: 412.6189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declear training and testing data set\n",
        "X_train10 = torch.tensor(np.concatenate((X[:10], X[-10:]), axis=0), dtype=torch.float32).reshape(-1, 1)\n",
        "Y_train10 = torch.tensor(np.concatenate((Y[:10], Y[-10:]), axis=0), dtype=torch.float32).reshape(-1, 1)\n",
        "X_test10 = torch.tensor(X[10:20], dtype=torch.float32).reshape(-1, 1)\n",
        "Y_test10 = torch.tensor(Y[10:20], dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Initialize the neural network\n",
        "net = Net()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 10\n",
        "# Train the neural network\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(X_train10.shape[0]):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(X_train10[i])\n",
        "        loss = criterion(outputs, Y_train10[i])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 10 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, X_train10.shape[0], loss.item()))\n",
        "\n",
        "# Compute the least square error for each data point in the test set\n",
        "Y_test10_pred = net(X_test10).detach().numpy().flatten()\n",
        "test_error = np.mean((Y_test10_pred - Y_test10.numpy())**2)\n",
        "print('Test Error: {:.4f}'.format(test_error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktK3UH4Cny0b",
        "outputId": "77ec7ab2-ad92-4c85-bbb8-b151fd6b2c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/20], Loss: 1177.1482\n",
            "Epoch [1/10], Step [20/20], Loss: 1142.0194\n",
            "Epoch [2/10], Step [10/20], Loss: 585.6099\n",
            "Epoch [2/10], Step [20/20], Loss: 214.1156\n",
            "Epoch [3/10], Step [10/20], Loss: 177.6659\n",
            "Epoch [3/10], Step [20/20], Loss: 82.7816\n",
            "Epoch [4/10], Step [10/20], Loss: 448.4232\n",
            "Epoch [4/10], Step [20/20], Loss: 19.8665\n",
            "Epoch [5/10], Step [10/20], Loss: 172.4289\n",
            "Epoch [5/10], Step [20/20], Loss: 114.9682\n",
            "Epoch [6/10], Step [10/20], Loss: 373.1484\n",
            "Epoch [6/10], Step [20/20], Loss: 38.6247\n",
            "Epoch [7/10], Step [10/20], Loss: 165.5328\n",
            "Epoch [7/10], Step [20/20], Loss: 102.0302\n",
            "Epoch [8/10], Step [10/20], Loss: 334.5544\n",
            "Epoch [8/10], Step [20/20], Loss: 43.4033\n",
            "Epoch [9/10], Step [10/20], Loss: 156.6917\n",
            "Epoch [9/10], Step [20/20], Loss: 112.0044\n",
            "Epoch [10/10], Step [10/20], Loss: 272.5663\n",
            "Epoch [10/10], Step [20/20], Loss: 68.4081\n",
            "Test Error: 112.4871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import mnist\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "sWeL3Brt3b1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(20, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.view(-1, 20) # flatten the input image\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vZPjwqZIoXcK"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the input images to 2D arrays\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "# Compute the first 20 PCA modes\n",
        "pca = PCA(n_components=20)\n",
        "pca.fit(x_train)\n",
        "\n",
        "# Transform the input data using the PCA modes\n",
        "x_train_pca = pca.transform(x_train)\n",
        "x_test_pca = pca.transform(x_test)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "x_train_pca = torch.tensor(x_train_pca, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "x_test_pca = torch.tensor(x_test_pca, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "FPB1Iy784tRM"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "train_set = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "test_set = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "# Compute the first 20 PCA modes of the digit images\n",
        "X_train = train_set.data.numpy().reshape(len(train_set), -1)\n",
        "pca = PCA(n_components=20)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_ratio = 0.8\n",
        "n_train_examples = int(train_ratio * len(train_set))\n",
        "n_valid_examples = len(train_set) - n_train_examples\n",
        "\n",
        "train_set_pca = torch.utils.data.TensorDataset(torch.tensor(X_train_pca[:n_train_examples], dtype=torch.float32),\n",
        "                                               train_set.targets[:n_train_examples])\n",
        "valid_set_pca = torch.utils.data.TensorDataset(torch.tensor(X_train_pca[n_train_examples:], dtype=torch.float32),\n",
        "                                               train_set.targets[n_train_examples:])\n",
        "test_set_pca = torch.utils.data.TensorDataset(torch.tensor(pca.transform(test_set.data.numpy().reshape(len(test_set), -1)), dtype=torch.float32),\n",
        "                                               test_set.targets)\n",
        "\n",
        "# Preprocess the data\n",
        "train_loader = torch.utils.data.DataLoader(train_set_pca, batch_size=64, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set_pca, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set_pca, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize the network and define the loss function and optimizer\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "# Test the network\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUy_zMVX2S94",
        "outputId": "4606ea79-9943-4ff7-ea05-4e2be9f795d4"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 114070757.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 77402999.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 37037248.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7273970.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Epoch [1/10], Step [100/750], Loss: 1.1868\n",
            "Epoch [1/10], Step [200/750], Loss: 0.8197\n",
            "Epoch [1/10], Step [300/750], Loss: 0.4574\n",
            "Epoch [1/10], Step [400/750], Loss: 0.8182\n",
            "Epoch [1/10], Step [500/750], Loss: 0.4705\n",
            "Epoch [1/10], Step [600/750], Loss: 0.5271\n",
            "Epoch [1/10], Step [700/750], Loss: 0.5561\n",
            "Epoch [2/10], Step [100/750], Loss: 0.3446\n",
            "Epoch [2/10], Step [200/750], Loss: 0.3786\n",
            "Epoch [2/10], Step [300/750], Loss: 0.3140\n",
            "Epoch [2/10], Step [400/750], Loss: 0.4578\n",
            "Epoch [2/10], Step [500/750], Loss: 0.5437\n",
            "Epoch [2/10], Step [600/750], Loss: 0.1200\n",
            "Epoch [2/10], Step [700/750], Loss: 0.5840\n",
            "Epoch [3/10], Step [100/750], Loss: 0.4432\n",
            "Epoch [3/10], Step [200/750], Loss: 0.2259\n",
            "Epoch [3/10], Step [300/750], Loss: 0.1558\n",
            "Epoch [3/10], Step [400/750], Loss: 0.1040\n",
            "Epoch [3/10], Step [500/750], Loss: 0.1367\n",
            "Epoch [3/10], Step [600/750], Loss: 0.1187\n",
            "Epoch [3/10], Step [700/750], Loss: 0.2256\n",
            "Epoch [4/10], Step [100/750], Loss: 0.1914\n",
            "Epoch [4/10], Step [200/750], Loss: 0.2561\n",
            "Epoch [4/10], Step [300/750], Loss: 0.3207\n",
            "Epoch [4/10], Step [400/750], Loss: 0.0524\n",
            "Epoch [4/10], Step [500/750], Loss: 0.1427\n",
            "Epoch [4/10], Step [600/750], Loss: 0.1933\n",
            "Epoch [4/10], Step [700/750], Loss: 0.3148\n",
            "Epoch [5/10], Step [100/750], Loss: 0.2058\n",
            "Epoch [5/10], Step [200/750], Loss: 0.2318\n",
            "Epoch [5/10], Step [300/750], Loss: 0.1745\n",
            "Epoch [5/10], Step [400/750], Loss: 0.1034\n",
            "Epoch [5/10], Step [500/750], Loss: 0.0681\n",
            "Epoch [5/10], Step [600/750], Loss: 0.2785\n",
            "Epoch [5/10], Step [700/750], Loss: 0.1174\n",
            "Epoch [6/10], Step [100/750], Loss: 0.2288\n",
            "Epoch [6/10], Step [200/750], Loss: 0.0623\n",
            "Epoch [6/10], Step [300/750], Loss: 0.1551\n",
            "Epoch [6/10], Step [400/750], Loss: 0.1595\n",
            "Epoch [6/10], Step [500/750], Loss: 0.2669\n",
            "Epoch [6/10], Step [600/750], Loss: 0.2025\n",
            "Epoch [6/10], Step [700/750], Loss: 0.1203\n",
            "Epoch [7/10], Step [100/750], Loss: 0.3411\n",
            "Epoch [7/10], Step [200/750], Loss: 0.1262\n",
            "Epoch [7/10], Step [300/750], Loss: 0.2041\n",
            "Epoch [7/10], Step [400/750], Loss: 0.3668\n",
            "Epoch [7/10], Step [500/750], Loss: 0.1888\n",
            "Epoch [7/10], Step [600/750], Loss: 0.1558\n",
            "Epoch [7/10], Step [700/750], Loss: 0.2215\n",
            "Epoch [8/10], Step [100/750], Loss: 0.1329\n",
            "Epoch [8/10], Step [200/750], Loss: 0.0024\n",
            "Epoch [8/10], Step [300/750], Loss: 0.0375\n",
            "Epoch [8/10], Step [400/750], Loss: 0.0425\n",
            "Epoch [8/10], Step [500/750], Loss: 0.0527\n",
            "Epoch [8/10], Step [600/750], Loss: 0.0764\n",
            "Epoch [8/10], Step [700/750], Loss: 0.3103\n",
            "Epoch [9/10], Step [100/750], Loss: 0.1080\n",
            "Epoch [9/10], Step [200/750], Loss: 0.1118\n",
            "Epoch [9/10], Step [300/750], Loss: 0.0089\n",
            "Epoch [9/10], Step [400/750], Loss: 0.2542\n",
            "Epoch [9/10], Step [500/750], Loss: 0.2010\n",
            "Epoch [9/10], Step [600/750], Loss: 0.2494\n",
            "Epoch [9/10], Step [700/750], Loss: 0.1618\n",
            "Epoch [10/10], Step [100/750], Loss: 0.2565\n",
            "Epoch [10/10], Step [200/750], Loss: 0.1492\n",
            "Epoch [10/10], Step [300/750], Loss: 0.0196\n",
            "Epoch [10/10], Step [400/750], Loss: 0.0229\n",
            "Epoch [10/10], Step [500/750], Loss: 0.2240\n",
            "Epoch [10/10], Step [600/750], Loss: 0.2611\n",
            "Epoch [10/10], Step [700/750], Loss: 0.2338\n",
            "Accuracy of the network on the 10000 test images: 95.66 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "zpDCu_m_67Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data from -1 to 1\n",
        "scaling = MinMaxScaler(feature_range=(-1,1)).fit(x_train_pca)\n",
        "x_train_pca = scaling.transform(x_train_pca)\n",
        "x_test_pca = scaling.transform(x_test_pca)\n",
        "\n",
        "\n",
        "# train SVM classifier\n",
        "svc = SVC(kernel='linear')\n",
        "svc.fit(x_train_pca, y_train)\n",
        "y_svc_pred = svc.predict(x_test_pca)\n",
        "print(f'SVM accuracy: {accuracy_score(y_test, y_svc_pred)}')\n",
        "\n",
        "# train decision tree classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(x_train_pca, y_train)\n",
        "y_dtc_pred = dtc.predict(x_test_pca)\n",
        "print(f'Decision tree accuracy: {accuracy_score(y_test, y_dtc_pred)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr8o_Xbex_0E",
        "outputId": "d2740612-0f93-4563-b5aa-bdd107d6d32f"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM accuracy: 0.9119\n",
            "Decision tree accuracy: 0.8459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM\n",
        "# Hyperparameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 5\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "# Create the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Instantiate the model and move it to the GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move the images and labels to the GPU\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print training statistics\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        # Move the images and labels to the GPU\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "id": "in3tpk4F3lfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd02b6a-4ef3-4963-e3d4-144f819cb627"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.3412\n",
            "Epoch [1/5], Step [200/600], Loss: 0.3392\n",
            "Epoch [1/5], Step [300/600], Loss: 0.1904\n",
            "Epoch [1/5], Step [400/600], Loss: 0.1392\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1644\n",
            "Epoch [1/5], Step [600/600], Loss: 0.2147\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0988\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1349\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0651\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0709\n",
            "Epoch [2/5], Step [500/600], Loss: 0.2002\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0865\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0677\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0879\n",
            "Epoch [3/5], Step [300/600], Loss: 0.1885\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0259\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0420\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0337\n",
            "Epoch [4/5], Step [100/600], Loss: 0.1512\n",
            "Epoch [4/5], Step [200/600], Loss: 0.1185\n",
            "Epoch [4/5], Step [300/600], Loss: 0.1003\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0623\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0244\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0085\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0465\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0514\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0638\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0449\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0455\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0585\n",
            "Accuracy of the model on the test images: 96.77 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}